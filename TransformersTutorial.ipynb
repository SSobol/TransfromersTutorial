{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TCXBzdPkCICK"
   },
   "source": [
    "# Transformers Tutorial: Case of next location prediction in mobility sequences\n",
    "\n",
    "Transformers are current state-of-the-art for sequence-to-sequence conversion tasks and generative AI. This lesson will illustrate the idea of transformers demonstrated over a case of next location prediction in mobility sequences. In this short Jupyter tutorial you will learn how to design, implement, train, and evaluate a transformer model in PyTorch.\n",
    "\n",
    "We shall\n",
    "* generate synthetic data - sequences of locations visited\n",
    "* process the data (train-test split, creating loaders for model training, splitting in batches)\n",
    "* desing, implement, train and evaluate (out-of-sample (OS) accuracy) the baseline recurrent neural network (RNN) model for the next location preduction\n",
    "* desing, implement, train and evaluate (OS accuracy) the transformer model the next location preduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qyz3I668DICy"
   },
   "source": [
    "## Generate the synthetic data\n",
    "\n",
    "We generate a series of synthetic sequences of locations visited by a random walker with memory. Relative probabilities of moving from one location to another exponentially decay with distance between locations, and are further depreciated for the locations recently visited, stimulating the random walker to explore new locations, rather than getting stuck into single locations of their immediate neighborhoods.\n",
    "\n",
    "The specific parameters of the process below won't be used by the transformer model - instead it will be up to it to figure out all the relevant features of the process in order to be able to predict the next location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "j-zyYooGot1I"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "P2HDOuLyob8b"
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "N = 1000  # number of sequences\n",
    "n = 100  # length of each sequence\n",
    "m = 30  # number of locations\n",
    "alpha = 15.0  # decay factor for distance\n",
    "recency_decay_depth = 10 #depth of random walkers previous location memory window\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "F0LAE8CyoOhR"
   },
   "outputs": [],
   "source": [
    "# Generate random 2D coordinates for the locations\n",
    "np.random.seed(0) #fix the random seed for reproducibility\n",
    "XY = np.random.normal(size = (m, 2)) #standard normal distribution for XY coordinates of the locations\n",
    "all_distances = np.linalg.norm(XY[:, np.newaxis, :] - XY[np.newaxis, :, :], axis=2) #define pairwide euclidian distances between locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "YiO9eR3uYSdd"
   },
   "outputs": [],
   "source": [
    "def generate_sequences(N, n, m, all_distances, recency_decay_depth, alpha): #function to generate synthetic mobility sequences\n",
    "    # N - number of sequences\n",
    "    # n - number of locations visited per sequence\n",
    "    # m - total number of locations\n",
    "    # all_distance - pairwise distances between locations\n",
    "    # alpha - parameter for probability exponential decay with distance\n",
    "    sequences = np.zeros((N, n), dtype=int)\n",
    "    visit_times = np.full((N, m), -np.inf)  # Initialize previous visit times per location with large negative values\n",
    "    current_locations = np.random.randint(m, size = N) #pick random initial locations\n",
    "\n",
    "    # Set initial locations and times\n",
    "    sequences[:, 0] = current_locations\n",
    "    visit_times[np.arange(N), current_locations] = 0\n",
    "\n",
    "\n",
    "    for t in range(1, n): #for each new step of the process\n",
    "        # Retrieve distances for current locations\n",
    "        current_distances = all_distances[current_locations, :]  #distances to possible next locations\n",
    "\n",
    "        # Calculate probability decay factors for recently visited locations\n",
    "        recency = np.maximum(recency_decay_depth + 2 + visit_times - t, 1)\n",
    "\n",
    "        # Calculate probabilities - exponential decay with distance plus additional decay for recently visited ones (current location can't be visited again, locations visited over the previous recency_decay_depth steps get their probabilities scaled down)\n",
    "        probabilities = (recency <= recency_decay_depth) * np.exp(-alpha * current_distances) / recency  # Shape (N, m)\n",
    "        probabilities /= probabilities.sum(axis=1, keepdims=True)  # Normalize probabilities\n",
    "\n",
    "        # Choose next locations based on probabilities\n",
    "        next_locations = np.array([np.random.choice(m, p=probabilities[i]) for i in range(N)])\n",
    "        sequences[:, t] = next_locations\n",
    "\n",
    "        # Update visit times and current locations\n",
    "        visit_times[np.arange(N), next_locations] = t\n",
    "        current_locations = next_locations\n",
    "\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "NPeBjI9fftGn"
   },
   "outputs": [],
   "source": [
    "# Generate training sequences\n",
    "sequences = generate_sequences(N, n, m, all_distances, recency_decay_depth, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i4W-WC9yZNJT",
    "outputId": "e58896ee-ee21-4d43-a8ec-ba6313ddac15"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([23, 25, 17, 22, 29, 19, 13, 19, 22, 19, 22, 29, 22, 19, 22, 29, 22,\n",
       "       19, 22, 29, 22, 19, 22, 19, 22, 29, 22, 19, 17, 28, 27, 15, 28,  4,\n",
       "       27, 28, 27, 15, 28, 27,  4, 27,  4, 27,  4, 27,  4, 15, 28, 15,  7,\n",
       "        6,  3,  6,  7, 28, 27,  4, 27, 15, 28, 27, 28, 27,  4, 15, 28, 27,\n",
       "        4, 27, 28, 27,  4, 27,  4, 17, 29, 22, 19, 13,  9, 11,  6,  3,  6,\n",
       "        7, 15, 28, 15, 28,  4, 27,  4, 27, 17, 19, 22, 29, 22, 19])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[0, :] #example of one of the generated sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "DE5Io-mNgyKJ"
   },
   "outputs": [],
   "source": [
    "#import pytorch-related libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "0_qNW0aHgvZY"
   },
   "outputs": [],
   "source": [
    "def prepare_data(sequences, split_ratio = [0.6, 0.8]): #prepare data to be fed to the model for training and validation; perform train - test split, split into batches\n",
    "    # Determine split index\n",
    "    num_sequences = sequences.shape[0]\n",
    "    split_index = [int(num_sequences * s) for s in split_ratio]\n",
    "\n",
    "    # Split the sequences into training-validation/test\n",
    "    train_sequences = sequences[:split_index[0]]\n",
    "    val_sequences = sequences[split_index[0]:split_index[1]]\n",
    "    test_sequences = sequences[split_index[1]:]\n",
    "\n",
    "\n",
    "    # Convert train-validation sequences to PyTorch datasets and loaders\n",
    "    # include only the\n",
    "    train_dataset = TensorDataset(torch.tensor(train_sequences[:, : -1]), torch.tensor(train_sequences[:, 1:]))\n",
    "    val_dataset = TensorDataset(torch.tensor(val_sequences[:, : -1]), torch.tensor(val_sequences[:, 1:]))\n",
    "    test_dataset = TensorDataset(torch.tensor(test_sequences[:, : -1]), torch.tensor(test_sequences[:, 1:]))\n",
    "\n",
    "    # DataLoader\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "    test_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "# Assuming `sequences` are ready and correctly shaped\n",
    "train_loader, val_loader, test_loader = prepare_data(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "r50iJewtgj-F"
   },
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module): #recurrent neural network (previous state-of-the-art baseline model to compare transformers with)\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size): #constructor, hidden_size (dimensionality of the hidden state) and num_layers (number of layers) are hyperparameters to specify\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True) #defined an RNN model\n",
    "        self.fc = nn.Linear(hidden_size, output_size) #define a final linear transform of an RNN's hidden state into the dimensionality of the output layer (number of locations)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, x): #feedforward step\n",
    "        out, _ = self.rnn(x) #convert input into hidden state over each token\n",
    "        out = self.fc(out) #convert hiddent state to output dimensionality\n",
    "        return out\n",
    "\n",
    "    def train_model(self, train_loader, val_loader, epochs=10, lr=0.001): #method for model training\n",
    "        criterion = nn.CrossEntropyLoss() #specify the loss function as a cross-entropy loss\n",
    "        optimizer = optim.Adam(self.parameters(), lr=lr) #specify Adam optimizer\n",
    "\n",
    "        for epoch in range(epochs): #iterate over epochs\n",
    "            self.train() #first call the superclass train\n",
    "            train_loss = 0\n",
    "            for data, targets in train_loader: #for each batch\n",
    "                data = data.float().unsqueeze(-1).to(self.device)  # Add feature dimension and send to the device\n",
    "                targets = targets.long().to(self.device) #send the target labels to the device\n",
    "                optimizer.zero_grad()\n",
    "                output = self(data) #feedforward step\n",
    "                loss = criterion(output.view(-1, m), targets.view(-1)) #define the loss function over the feedforward output\n",
    "                loss.backward() #backpropagate the loss gradient\n",
    "                optimizer.step() #update the weights\n",
    "                train_loss += loss.item() #aggregate the values of the loss over batches\n",
    "\n",
    "\n",
    "            # Assess the loss over validation data (without training)\n",
    "            self.eval()\n",
    "            valid_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for data, targets in val_loader:\n",
    "                    data = data.float().unsqueeze(-1).to(self.device)\n",
    "                    targets = targets.long().to(self.device)\n",
    "                    output = self(data)\n",
    "                    loss = criterion(output.view(-1, m), targets.view(-1))\n",
    "                    valid_loss += loss.item()\n",
    "\n",
    "            print(f'Epoch {epoch+1}, Train Loss: {train_loss/len(train_loader)}, Validation Loss: {valid_loss/len(val_loader)}')\n",
    "\n",
    "    def evaluate(self, test_loader): #method for accuracy evaluation over a given dataset\n",
    "        self.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for data, targets in test_loader: #iterate over batches\n",
    "                data = data.float().unsqueeze(-1).to(self.device)\n",
    "                targets = targets.long().to(self.device)\n",
    "                outputs = self(data)\n",
    "                predicted = outputs.argmax(dim=2) #discretize output - take the most likely location token\n",
    "                total += targets.size(0) * targets.size(1)\n",
    "                correct += (predicted == targets).sum().item() #count the number of label matches between the predicted and actual location token\n",
    "\n",
    "        accuracy = correct / total\n",
    "        print(f'Accuracy: {accuracy}')\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "exjYkLQYgmg5",
    "outputId": "4db48b0f-eb2b-443e-f31e-c91ecd8c10fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 2.92254460485358, Validation Loss: 2.6295317241123746\n",
      "Epoch 2, Train Loss: 2.48076195465891, Validation Loss: 2.3950676918029785\n",
      "Epoch 3, Train Loss: 2.2627851963043213, Validation Loss: 2.2028953347887312\n",
      "Epoch 4, Train Loss: 2.0916713287955835, Validation Loss: 2.0129064321517944\n",
      "Epoch 5, Train Loss: 1.9276610863836188, Validation Loss: 1.8194690772465296\n",
      "Epoch 6, Train Loss: 1.774414991077624, Validation Loss: 1.6585581302642822\n",
      "Epoch 7, Train Loss: 1.662080256562484, Validation Loss: 1.5642338820866175\n",
      "Epoch 8, Train Loss: 1.5855204557117664, Validation Loss: 1.4963739940098353\n",
      "Epoch 9, Train Loss: 1.5312239810040122, Validation Loss: 1.444969722202846\n",
      "Epoch 10, Train Loss: 1.482482715656883, Validation Loss: 1.4023301431110926\n",
      "Epoch 11, Train Loss: 1.4404163109628778, Validation Loss: 1.361840350287301\n",
      "Epoch 12, Train Loss: 1.3999385018097728, Validation Loss: 1.3279962028775896\n",
      "Epoch 13, Train Loss: 1.36403825408534, Validation Loss: 1.2958445719310216\n",
      "Epoch 14, Train Loss: 1.3357646967235364, Validation Loss: 1.2695054156439645\n",
      "Epoch 15, Train Loss: 1.3085477916817916, Validation Loss: 1.2481186900820052\n",
      "Epoch 16, Train Loss: 1.288194963806554, Validation Loss: 1.2293161834989275\n",
      "Epoch 17, Train Loss: 1.2682474412416156, Validation Loss: 1.2117033856255668\n",
      "Epoch 18, Train Loss: 1.2542294138356258, Validation Loss: 1.1967172452381678\n",
      "Epoch 19, Train Loss: 1.236753219052365, Validation Loss: 1.1846287591116769\n",
      "Epoch 20, Train Loss: 1.2242986152046604, Validation Loss: 1.1721848590033395\n",
      "Epoch 21, Train Loss: 1.21219329457534, Validation Loss: 1.1628146427018302\n",
      "Epoch 22, Train Loss: 1.2007713317871094, Validation Loss: 1.1513344134603227\n",
      "Epoch 23, Train Loss: 1.1927898055628727, Validation Loss: 1.1421094196183341\n",
      "Epoch 24, Train Loss: 1.1830916059644598, Validation Loss: 1.1330321516309465\n",
      "Epoch 25, Train Loss: 1.1736891646134227, Validation Loss: 1.1279020820345198\n",
      "Epoch 26, Train Loss: 1.1658107293279547, Validation Loss: 1.1167436923299516\n",
      "Epoch 27, Train Loss: 1.1550978422164917, Validation Loss: 1.1130581838744027\n",
      "Epoch 28, Train Loss: 1.1497764210951955, Validation Loss: 1.102925717830658\n",
      "Epoch 29, Train Loss: 1.1412221318797062, Validation Loss: 1.0961484057562692\n",
      "Epoch 30, Train Loss: 1.134254725355851, Validation Loss: 1.091985889843532\n"
     ]
    }
   ],
   "source": [
    "# Model initialization\n",
    "torch.manual_seed(0)\n",
    "model = RNNModel(input_size = 1, hidden_size = 64, num_layers = 2, output_size = m)\n",
    "\n",
    "# Assuming train_loader and val_loader have been defined\n",
    "model.train_model(train_loader, test_loader, epochs = 30, lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cyWyQeOkgof6",
    "outputId": "c61daed3-f13d-44b9-d4d8-1b1baa1b0b65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6044949494949495\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6044949494949495"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming test_loader is defined for the new sequence or a separate test set\n",
    "model.evaluate(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BMRuyw7_jrPs"
   },
   "source": [
    "So we reach the OS accuracy of over 60% with relatively little effort designing and training the model! Not so bad for predicting a random walk... Let's see if a transformer model can do it any better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "3cUGvVUJq6mR"
   },
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module): #transfomrer model\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, nhead, max_len=1000): #initiatize with hyperparameters - dimensionality of the hidden layer (embedding), number of attention heads, number of layers, output_size - number of locations\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size) #introduce the trainable token embedding for all the locations\n",
    "        self.output_size = output_size #number of locations\n",
    "        self.positional_encoding = self._generate_positional_encoding(hidden_size, max_len) #add positional embedding\n",
    "        self.transformer = nn.TransformerEncoder( #introduce the transformer model\n",
    "            nn.TransformerEncoderLayer(d_model = hidden_size, nhead = nhead, dim_feedforward = hidden_size * 4),\n",
    "            num_layers=num_layers)\n",
    "        self.out = nn.Linear(hidden_size, output_size) #add a linear layer to convert transformer output into next token probability distribution (subject to further softmax nodmalization)\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, x): #feedworward step\n",
    "      # x: [batch_size, seq_length]\n",
    "      x = self.embedding(x)  # Transform input to embeddings: [batch_size, seq_length, hidden_size]\n",
    "\n",
    "      # Ensure positional_encoding is broadcastable over the batch dimension\n",
    "      # x.size(1) gives the sequence length, positional_encoding needs to be sliced accordingly\n",
    "      pos_encoding = self.positional_encoding[:x.size(1), :].unsqueeze(0)  # [1, seq_length, hidden_size]\n",
    "      x = x + pos_encoding  #Add positional encoding to each batch of token embeddings\n",
    "\n",
    "      x = x.permute(1, 0, 2)  # Transformer expects [seq_len, batch_size, features]\n",
    "      attn_mask = torch.triu(torch.ones(x.size(0), x.size(0)) * float('-inf'), diagonal=1).to(self.device)\n",
    "      x = self.transformer(x, mask = attn_mask)\n",
    "      x = x.permute(1, 0, 2)  # Back to [batch_size, seq_len, features]\n",
    "      return self.out(x) #apply a final linear output layer and return the output\n",
    "\n",
    "    def _generate_positional_encoding(self, hidden_size, max_len): #trigonometric positional embedding\n",
    "        pe = torch.zeros(max_len, hidden_size)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, hidden_size, 2).float() * (-np.log(10000.0) / hidden_size))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pe.to(self.device)\n",
    "\n",
    "    def train_model(self, train_loader, val_loader, epochs=10, lr=0.001): #method for model training\n",
    "        criterion = nn.CrossEntropyLoss() #introduce cross-entropy loss\n",
    "        optimizer = optim.Adam(self.parameters(), lr=lr) #introduce adam optimizer\n",
    "\n",
    "        for epoch in range(epochs): #iterate through epochs\n",
    "            self.train()\n",
    "            train_loss = 0\n",
    "            for data, targets in train_loader: #iterate through batches\n",
    "                data = data.long().to(self.device)\n",
    "                targets = targets.long().to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "                output = self(data) #feedforward loop\n",
    "                loss = criterion(output.view(-1, self.output_size), targets.view(-1)) #loss function\n",
    "                loss.backward() #backpropagate loss gradient\n",
    "                optimizer.step() #update weights towards the gradient\n",
    "                train_loss += loss.item() #accumulate batch loss\n",
    "\n",
    "            # Complute validation (OS) loss\n",
    "            self.eval()\n",
    "            valid_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for data, targets in val_loader:\n",
    "                    data = data.long().to(self.device)\n",
    "                    targets = targets.long().to(self.device)\n",
    "                    output = self(data)\n",
    "                    loss = criterion(output.view(-1, self.output_size), targets.view(-1))\n",
    "                    valid_loss += loss.item()\n",
    "\n",
    "            train_loss /= len(train_loader)\n",
    "            valid_loss /= len(val_loader)\n",
    "            \n",
    "            print(f'Epoch {epoch+1}, Train Loss: {train_loss}, Validation Loss: {valid_loss}')\n",
    "        return train_loss, valid_loss    \n",
    "\n",
    "    def evaluate(self, test_loader): #accuracy evaluation for the given dataset\n",
    "        self.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad(): #iterate through all the batches\n",
    "            for data, targets in test_loader:\n",
    "                data = data.long().to(self.device)\n",
    "                targets = targets.long().to(self.device)\n",
    "                outputs = self(data)\n",
    "                predicted = outputs.argmax(dim=2)\n",
    "                total += targets.size(0) * targets.size(1)\n",
    "                correct += (predicted == targets).sum().item()\n",
    "\n",
    "        accuracy = correct / total\n",
    "        print(f'Accuracy: {accuracy}')\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "anRuc4C_q6w9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stanislav/opt/anaconda3/envs/STTN/lib/python3.9/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "# Model parameters\n",
    "torch.manual_seed(0)\n",
    "input_size = 1  # not used in our Transformer model, kept for compatibility\n",
    "hidden_size = 64  # dimension of the embedding and hidden layers\n",
    "num_layers = 2  # number of transformer layers\n",
    "nhead = 8  # number of heads in multi-head attention mechanisms\n",
    "\n",
    "# Create the Transformer model instance\n",
    "model2 = TransformerModel(input_size, hidden_size, num_layers, m, nhead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TPjEKlWEq6z0",
    "outputId": "b83df953-3809-48bf-fd68-9f574e54e65a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 2.266926332523948, Validation Loss: 1.4887442759105138\n",
      "Epoch 2, Train Loss: 1.3512907279165167, Validation Loss: 1.139384354863848\n",
      "Epoch 3, Train Loss: 1.1304840288664166, Validation Loss: 1.0026372160230363\n",
      "Epoch 4, Train Loss: 1.0307391756459285, Validation Loss: 0.939822963305882\n",
      "Epoch 5, Train Loss: 0.9808928025396246, Validation Loss: 0.9090287429945809\n",
      "Epoch 6, Train Loss: 0.9558532426231786, Validation Loss: 0.8880487339837211\n",
      "Epoch 7, Train Loss: 0.9366461258185538, Validation Loss: 0.8734409894262042\n",
      "Epoch 8, Train Loss: 0.9222105804242586, Validation Loss: 0.8664211119924273\n",
      "Epoch 9, Train Loss: 0.9132043468324762, Validation Loss: 0.8567688379968915\n",
      "Epoch 10, Train Loss: 0.906146272232658, Validation Loss: 0.8531199097633362\n",
      "Epoch 11, Train Loss: 0.8990879780367801, Validation Loss: 0.8465749110494342\n",
      "Epoch 12, Train Loss: 0.8913789140550714, Validation Loss: 0.8413337809698922\n",
      "Epoch 13, Train Loss: 0.8858821548913655, Validation Loss: 0.8371955156326294\n",
      "Epoch 14, Train Loss: 0.8803300230126632, Validation Loss: 0.834816745349339\n",
      "Epoch 15, Train Loss: 0.878598407695168, Validation Loss: 0.8331555894442967\n",
      "Epoch 16, Train Loss: 0.8742393443458959, Validation Loss: 0.8275863868849618\n",
      "Epoch 17, Train Loss: 0.8680651783943176, Validation Loss: 0.8234678166253226\n",
      "Epoch 18, Train Loss: 0.8663817487264934, Validation Loss: 0.8246028423309326\n",
      "Epoch 19, Train Loss: 0.866033673286438, Validation Loss: 0.8207596370152065\n",
      "Epoch 20, Train Loss: 0.8608758731892234, Validation Loss: 0.8207876937729972\n",
      "Epoch 21, Train Loss: 0.8595404656309831, Validation Loss: 0.8186436806406293\n",
      "Epoch 22, Train Loss: 0.8569558764758863, Validation Loss: 0.8183083363941738\n",
      "Epoch 23, Train Loss: 0.8540517907393607, Validation Loss: 0.8162361979484558\n",
      "Epoch 24, Train Loss: 0.8535324366469133, Validation Loss: 0.8147273659706116\n",
      "Epoch 25, Train Loss: 0.8496072072731821, Validation Loss: 0.8116521068981716\n",
      "Epoch 26, Train Loss: 0.8481222422499406, Validation Loss: 0.8140049747058323\n",
      "Epoch 27, Train Loss: 0.8474198673900805, Validation Loss: 0.8145739776747567\n",
      "Epoch 28, Train Loss: 0.8465758122895893, Validation Loss: 0.8119856204305377\n",
      "Epoch 29, Train Loss: 0.8446738343489798, Validation Loss: 0.8134213856288365\n",
      "Epoch 30, Train Loss: 0.8428314547789725, Validation Loss: 0.8134488633700779\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8428314547789725, 0.8134488633700779)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model2.train_model(train_loader, test_loader, epochs = 30, lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "saYr8k2lq62b",
    "outputId": "5acf68d2-84e0-430e-e5fa-468b8ec693ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6781313131313131\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6781313131313131"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.evaluate(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yQiHByAgGT1X"
   },
   "source": [
    "The 68% OS accuracy looks much better! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C2DvES78vcxN"
   },
   "source": [
    "## Excercise.\n",
    "Try different values of hyperparameters (64 triplets): num_layers = 1, 2, 3, 4; nhead = 1, 2, 4, 8; hidden_size = 16, 32, 64, 128. Compare performance (loss) after 10 epochs over the validation set. Pick the best configuration, and report the corresponding model's performance (loss and accuracy) over the test set after 30 training epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
